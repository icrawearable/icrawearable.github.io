<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition | Towards Generalist Robots - CoRL23</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Towards Generalist Robots - CoRL23" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition","name":"Towards Generalist Robots - CoRL23","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Towards Generalist Robots - CoRL23" />
</head>
<body><header class="site-header">
  <div class="wrapper"><!-- <a class="site-title" rel="author" href="/">Towards Generalist Robots - CoRL23</a> --><a class="site-title" rel="author" href="/">Towards Generalist Robots - CoRL23</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/speakers/">Speakers</a><a class="page-link" href="/schedule/">Schedule</a><a class="page-link" href="/organizers/">Organizers</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 class="page-heading" align="center"><p style="font-size: 1.3em">Towards Generalist Robots: <br> Learning Paradigms for Scalable Skill Acquisition</p></h1><p style="text-align: center;"><font size="5">Conference on Robot Learning 2023, Atlanta, USA</font></p>
<p style="text-align: center;"><font size="5">Monday, Nov 6th, 2023
</font></p>

<p>We have witnessed very impressive progress in large-scale and multi-modal foundation/generative models in recent months. We believe making use of such models in a reasonable way could really enable robots to acquire diverse skills. In the recent <a href="https://arxiv.org/pdf/2305.10455.pdf">white paper</a>, we discussed how we can automate the whole pipeline for robotic skill learning, from low-level asset generation, texture generation, to high-level scene, task and reward generation. Once we obtain such a diverse suite of tasks and environments, we can offload policy training to RL of trajectory optimization to solve all the generated low-level tasks, and finally distill all the learned closed-loop policy into a unified policy model. Apart from scaling up in simulation, how to use real-world data more effectively is another promising research direction. Real-world human demonstrations can be found at scale, but typically only provides spatial trajectory information and doesn’t advise how to recover from error compounding during policy rollout.</p>

<p><img src="/assets/img/banner.jpg" alt="" /></p>

<p>Motivated by these observations and thoughts, this workshop seeks to discuss and compare the advantages and limitations of different paradigms for scaling up skill learning: scaling up simulation, leveraging generative models, exploiting unstructured passive human demonstration, scaling up structured demonstration collection in the real world, etc.</p>

<p>We aim to discuss questions including, but not limited to:</p>
<ul>
  <li>Is automating task and scene generation in simulation a valid paradigm to pursue?</li>
  <li>Are today’s simulators capable of simulating diverse physical phenomena that would be encountered in real-world robotic tasks?</li>
  <li>Is sim2real transfer a major bottleneck? What are the potential solutions to it?</li>
  <li>What are the pros and cons of collecting learning from real-world data compared to simulated data?</li>
  <li>How to make collecting and learning from real-world data more efficient and effective?</li>
  <li>Is there a way to combine the strengths of both simulated and real-world data for learning generalist robots?</li>
</ul>

<p> 
 </p>

<h3 id="call-for-papers">Call for papers</h3>

<p>We invite submissions including but not limited to the following topics:</p>

<ul>
  <li>
    <p>Scaling up robotic data with generative AI</p>
  </li>
  <li>
    <p>Exploiting knowledge from vision and language foundation models for robotics</p>
  </li>
  <li>
    <p>Scalable skill learning in simulation and sim-to-real transfer</p>
  </li>
  <li>
    <p>Scalable real-world data collection and robot learning</p>
  </li>
  <li>
    <p>Learning robotic foundation models by leveraging internet-scale data</p>
  </li>
</ul>

<p><strong>Important Dates</strong></p>
<ul>
  <li>
    <p><strong>Paper submission open</strong>: 2023/09/07</p>
  </li>
  <li>
    <p><strong>Paper submission deadline</strong>: 2023/10/06 (Extended to 2023/10/08 AoE)</p>
  </li>
  <li>
    <p><strong>Notification of acceptance</strong>: 2023/10/20</p>
  </li>
  <li>
    <p><strong>Workshop date</strong>: 2023/11/06</p>
  </li>
  <li>
    <p><strong>Submission portal</strong>: <a href="https://openreview.net/group?id=robot-learning.org/CoRL/2023/Workshop/TGR">CoRL 2023 Workshop TGR (OpenReview)</a>.</p>
  </li>
</ul>

<p>We expect submissions with 4 - 8 pages for the main content, with no limit on references/appendices. Submissions are suggested to use the <a href="https://drive.google.com/file/d/1Ksqw_9OMiCKEiGmoaqn3QCuqcRpgtZ5a/view">CoRL template</a> and must be anonymized. All papers will be peer-reviewed in a double-blind manner. We welcome both unpublished original contributions and recently published relevant works. Accepted papers will be presented in the form of posters, with several papers being selected for spotlight sessions.</p>

<p><strong>Contact</strong></p>

<p>If you have any questions, please contact us at: generalistrobots@gmail.com.</p>

<p> 
 </p>
<h3 id="invited-speakers">Invited Speakers</h3>
<p> </p>

<div class="grid">
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/shuran_song.jpeg" width="200" />
        <figcaption><b><a href="https://shurans.github.io/">Shuran Song</a></b><br />Stanford University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/sergey_levine.png" width="200" /> 
        <figcaption><b><a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a></b><br />UC Berkeley</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/abhinav_gupta.jpeg" width="200" />
        <figcaption><b><a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/dhruv_batra.jpeg" width="200" />
        <figcaption><b><a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a></b><br />Georgia Tech &amp; Meta AI</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/ming_lin.jpeg" width="200" />
        <figcaption><b><a href="https://www.cs.umd.edu/~lin/">Ming C. Lin</a></b><br />University of Maryland, College Park</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/karen_liu.png" width="200" />
        <figcaption><b><a href="https://profiles.stanford.edu/c-karen-liu">Karen Liu</a></b><br />Stanford University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/chuang_gan.jpeg" width="200" />
        <figcaption><b><a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a></b><br />UMass Amherst &amp; MIT-IBM AI Lab</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/he_wang.jpeg" width="200" />
        <figcaption><b><a href="https://hughw19.github.io/">He Wang</a></b><br />Peking University</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/dieter.jpg" width="200" />
        <figcaption><b><a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></b><br />University of Washington &amp; Nvidia</figcaption>
        </figure>
    </div>
    <div class="grid-item">
        <figure>
        <img src="assets/img/speakers/davide_scaramuzza.jpeg" width="200" />
        <figcaption><b><a href="https://rpg.ifi.uzh.ch/people_scaramuzza.html">Davide Scaramuzza</a></b><br />University of Zurich</figcaption>
        </figure>
    </div>
</div>

<p> </p>

<h3 id="organizers">Organizers</h3>
<p> </p>

<div class="grid">
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/zhou_xian.webp" width="200" />
        <figcaption><b><a href="https://www.zhou-xian.com/">Zhou Xian</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/theophile_gervet.jpeg" width="200" /> 
        <figcaption><b><a href="https://theophilegervet.github.io/">Theophile Gervet</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/zhenjia_xu.jpeg" width="200" />
        <figcaption><b><a href="https://www.zhenjiaxu.com/">Zhenjia Xu</a></b><br />Columbia University</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/yi_ling_qiao.jpeg" width="200" />
        <figcaption><b><a href="https://ylqiao.net/">Yi-Ling Qiao</a></b><br />UMD, College Park</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/tsun_hsuan_wang.jpeg" width="200" />
        <figcaption><b><a href="https://zswang666.github.io/">Tsun-Hsuan Wang</a></b><br />MIT</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/yian_wang.JPG" width="200" />
        <figcaption><b><a href="https://generalist-robots.github.io/">Yian Wang</a></b><br />UMass Amherst</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/chen_wang.jpeg" width="200" />
        <figcaption><b><a href="https://www.chenwangjeremy.net/">Chen Wang</a></b><br />Stanford University</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/katerina_fragkiadaki.png" width="200" />
        <figcaption><b><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/david_held.png" width="200" />
        <figcaption><b><a href="https://davheld.github.io/">David Held</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/chris_atkeson.jpeg" width="200" />
        <figcaption><b><a href="http://www.cs.cmu.edu/~cga/">Chris Atkeson</a></b><br />CMU</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/josh_tenenbaum.jpeg" width="200" />
        <figcaption><b><a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a></b><br />MIT</figcaption>
        </figure>
    </div>
    <div class="gridorg-item">
        <figure>
        <img src="assets/img/organizers/daniela_rus.jpeg" width="200" />
        <figcaption><b><a href="https://danielarus.csail.mit.edu/">Daniela Rus</a></b><br />MIT</figcaption>
        </figure>
    </div>
</div>




  </div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!-- <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p> -->
      </div>
      <div class="footer-col">
        <p></p>
        <p></p>
        
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

    <div class="footer-col" align="center">
      <a href="https://github.com/krrish94/sniffle-workshop">Template</a>
    </div>

  </div>

</footer>
</body>

</html>
